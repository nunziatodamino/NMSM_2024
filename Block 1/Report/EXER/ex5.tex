\chapter*{Lecture 5: Ising 2D}

The Ising model is, by earsay, the undisputed king of the Monte Carlo simulations. Its simplicity, combined with the richness of its behavior, 
makes it an ideal playground for testing and developing important computational techniques as the Metropolis algorithm, that we will 
see in the following. \\
In this exercise we'll simulate the 2D Ising model on a square lattice, using the Glauber dynamics (local spin flip) and the Metropolis algorithm
for the configuration evolution. We'll then evaluate typical system observables at equilibrium for different temperatures and different system sizes.

\subsection{Introduction}

We recall briefly some fundamental theoretical facts about the Ising model that we'll use through the simulation.
We begin considering a generic lattice where we place $N$ spins of the type $\sigma_i  = \pm 1$. If one fixes an origin on the lattice, and consider
from this the set of positions $\{\bar{r}_0, \bar{r}_1, \dots \bar{r}_{N-1}\}$ of the other lattice points, then is possible 
to define a configuration $\CC = \{\sigma_{\bar{r}_0}, \dots \sigma_{\bar{r}_{N-1}}\}$ as the set of spin values assumed on each lattice point. \\
We define the energy of configuration $\CC$ as:

$$
\mathcal{H}(C) = -J \sum_{\langle \bar{r_i}\bar{r_j} \rangle} \sigma_{\bar{r_i}} \sigma_{\bar{r_i}}
$$

Where the symbol $\sum_{\langle \bar{r_i}\bar{r_j} \rangle}$ implies that the sum is done only over the nearest neighbors. In our simulation 
we'll consider for simplicity $J=1$. \\

In the case of the square lattice, this model can be solved exactly in the limit of an infinite lattice \cite{Onsager-1944}. \\
The result for the critical temperature and the expected magnetisation in function
of the temperature are reported in the following.
We have that the magnetization has the following function:

$$
M = 
\begin{cases}
(1-\sinh^{-4}(2\beta J))^{1/8} \quad&T<T_c\\
0 &T \geq T_c    
\end{cases}
$$

Where holds for the critical temperature the relation:

$$ \frac{k_BT_c}{J} = \frac{2}{\ln(1 +\sqrt{2})} \simeq 2.269 $$

Again, to simplify our simulation, we will consider $k_B = 1$ (natural units). \\

Now we can begin discussing the simulation details .\\
We begin by generating a random configuration at the start of the simulation such that the probability to have a spin up or a spin down is
$\frac{1}{2}$. With this choice, if the system evolves with $T<T_C$ can either reach an equilibrium configuration where all spins are up or down ,
according to the phase diagram. \\
Notice that if we, for whatever reason, would like to reach an equilibrium configuration respect to the other we can do that by skewing
the picking probability in favor of what we desire. \\
Then we go from this configuration $\CC$ to a new configuration $\CC'$ performing $N$ spin local moves, 
where the algorithm to perform a single spin flip and decide to accept it or not, is described in the following:

\begin{algorithm}
    \caption{Metropolis Spin-Flip Dynamics}
    \begin{algorithmic}[1]
        \State \((i, j) \sim \text{U}(0, \text{length})\)
        \State \(\text{neighbor\_sum} \gets \sum \left( \text{old\_configuration}[\text{neighbor}] \; \text{for each neighbor in} \; \text{neighbors\_list}[(i, j)] \right)\)
        \State \(\Delta E \gets 2J \times \text{old\_configuration}[i, j] \times \text{neighbor\_sum}\)
        \State \(\text{new\_configuration} \gets \text{old\_configuration}\)
        \State \(\text{new\_configuration}[i, j] \gets -\text{old\_configuration}[i, j]\) \Comment{Flip the spin at \((i, j)\)}
        \If{\(\Delta E \leq 0\)}
            \State \textbf{Return} \(\text{new\_configuration}\)
        \Else
            \State \(u \sim U(0,1)\)
            \If{\(u \leq e^{-\beta \Delta E}\)}
                \State \textbf{Return} \(\text{new\_configuration}\)
            \Else
                \State \textbf{Return} \(\text{old\_configuration}\)
            \EndIf
        \EndIf
    \end{algorithmic}
    \label{metropolis_spin_flip}
\end{algorithm}

Notice that in order to reduce the computation time a neighbor list for each atom, considering the periodic boundary condition, 
is evaluated using an hash table at the start of the program, so that the algorithm can use it as input. This can be implemented
 easily in Python using a dictionary. \\
In our simulation we'll define 1 Monte Carlo timestep as the number of steps in which we perform $N$ local spin flip moves. \\

\subsection{Warm-up run} 
Given that we're starting from a random configuration we'll need some time before reaching the thermal equilibrium, 
i.e. the system we'll need to evolve for a certain number of Monte Carlo timesteps that we'll call $\tau_{eq}$ before we start to evaluate any mean observable. \\
This process is called thermalization and is a fundamental part in any simulation of this kind. \\
There are different recipes to find $\tau_{eq}$ for a given system, either qualitative or quantitative. One possible way can be to plot the
observables of interest against time and see qualitatively when they tend to stabilize. One can, on top of this, introduce a quantitative criterion,
say that the variance per observable must be less than a predetermined value $\alpha$. This approach, even if simple, has a clear caveat : it establish a lower
bound for the observables time series but not an upper bound. \\
Another approach is to evaluate at this stage the autocorrelation time for each observable. If the process is stationary or weakly stationary this number is 
well defined through the time series ($C_O(s, s+t) = C_O=(0, t)\ \forall s, \forall O$ for these kind of processes). Then we can use this number as a discard criterion 
(the common recipe is $\tau_{eq} \simeq 20 \tau$, see \cite{Sokal1997}) but moreover we can use it in establishing an upper bound for the simulation. \\
We'll discuss in detail in the autocorrelation paragraph that errors for correlated measures are of the order $\sim (\frac{\tau}{N})^{1/2}$, so in order to 
achieve, for example, an error of the order $ \sim 0.01$ for our measurements or better, $N > 10000\tau$ . \\

A point of interest is that, technically, there is no need to discard any data. The initial data (biased estimation) leads to a systematic error $\sim \frac{\tau}{N}$,
while the statistical errors will be of the order $\sim (\frac{\tau}{N})^{1/2}$, which is larger. In practice, given that the systematic error can be big, 
and we can let it be 0 with no cost, we discard the initial transient. \\ 

We then start the thermalization process with a warmup run where the maximum number of iterations is not the final one, and plot for each temperature and for 
each square lattice the 2 observables of interest (energy and magnetisation per spin) against the simulation time. \\

Here we have 2 possible roads: the quantitative one, i.e. if an observable stabilize to a constant value (apart from the statistical noise) 
we assume that from there on the process is stationary, or the quantitative one, i.e. to effectively prove that the time series is, from some $\tau_{equilibrium}$ on ,
stationary or weakly stationary. \\
In order to prove this we can naively see numerically if the time series has constant mean, constant variance, and constant autocorrelation time in a certain tolerance 
range. While the implementation of this procedure is straightforward, is extremely susceptible to numerical instability, 
so in the code just a proof of concept version is presented, because its optimization is behind the scope of this analysis. \\
If one want to use a more robust and reliable way to prove this, then an Augmented Dickey-Fuller (ADF) test should be performed on the time series. \\
Due to time limitation, we opted this time for the qualitative approach. \\

Once the time series are evaluated for each lattice, we discard the initial transient and evaluate for each observable the autocorrelation time, and then take the maximum value between the 2.
These $\tau$, that are of the same order of the final $\tau$ values reported in the tables \ref{lec5:results_side25}, \ref{lec5:results_side50}, \ref{lec5:results_side100} where used 
to estimate how much the final simulation could run, i.e. the final upper bound. 

\subsection{Autocorrelation} In Monte Carlo simulations, successive samples for each observable are often correlated due to the nature of the sampling algorithm. \\
How much they are correlated is described by the autocorrelation function which can be showed that for large $t$ goes to 0 as an exponential:

$$ C_O(T) \sim e^{-t/\tau^O_{exp}} $$

Where $\tau^O_{exp}$ is called autocorrelation time. \\
In literature \cite{Weigel-2010} one can also find the definition of the so called integrated autocorrelation correlation time:

$$ \tau^O_{int} = \frac{1}{2} + \sum_{t=1}^N \frac{C_O(t)}{C_O(0)} $$

This definition makes $\tau^O_{exp} \sim \tau^O_{int}$, and moreover one can prove \cite{Weigel-2010} that one is the upper bound of the other, specifically $\tau^O_{int} \leq \tau^O_{exp}$. \\
In the case of correlated data one should correct the variance by the number of uncorrelated data \cite{Weigel-2010} :

$$ \sigma^2(O)_{corr} = \frac{\sigma^2(O)}{\frac{N}{2 \tau_{int}^O}} $$

In our analysis, even if we recognize that these 2 times technically play different roles in the analysis, we just evaluate $\tau^O_{exp}$, and take this as an overestimate of $\tau^O_{int}$. So, from now on $\tau^O_{int} \sim \tau^O_{exp} \equiv \tau_{O}$

\subsection{Simulation}

In our simulation, we found correlation times of different orders of magnitude, as seen in tables \ref{lec5:results_side25}, \ref{lec5:results_side50}, \ref{lec5:results_side100}, where 
we have an higher correlation time when we are near the critical temperature for each dimension of the square lattice, due to the critical slowing down phenomenon. \\
In order to find a tradeoff between computation time and accuracy, we choose, $t_{max} = 10^5$ . This is a good choice in general but leads in the worst case scenario, i.e. $\tau \sim 400$ , to 
a $t_{max} = 250 \tau$ and subsequently to an error $\sim (\frac{\tau}{t_{max}})^{1/2}$ of the order of $0.2$, which for us is terrible giving that we're estimating quantities of 
the same order. \\
In order to mitigate this one should propose a simulation of the order $t_{max} = 10^6 - 10^9$ , to be safe. \\

We now comment the results obtained in tables \ref{lec5:results_side25}, \ref{lec5:results_side50}, \ref{lec5:results_side100}. Note that all observables are evaluated 
per spin and the heat capacity $C_V$ and the magnetic susceptibility $\chi_M$ are evaluated via block averaging (\ref{block_averaging}) to be able to estimate their errors. 

\begin{itemize}
    \item \textbf{Magnetisation per spin} : We see for each dimension a transition from a completly magnetized state to a non-magnetized one. \\ In the neighborhood of the critical temperature for each dimension the lower bound (0.95\ $T_c$)
    is consistent with the theoretical prediction and the upper bound $(1.05 T_c)$ can be considered consistent given the premises on the large error made at the start of the paragraph.

    \item \textbf{Energy per spin}: We have that the lowest energy state for the square lattice Ising model is $E = -2JN$ (all spins up) , while if all the spins are random $(T \to +\infty)$, we have $E \simeq 0$. We see clearly 
    this trend respected for all dimensions.

    \item \textbf{Heat capacity per spin}: We see that we obtain a result that is almost invariant by the choice of the dimension. This may seems an error at first but we recall that we are 
    analyzing a finite size Ising model, and this has the effect to shift the $T_c$ from the theoretical results when the dimensions changes. \\
    To have a proper description of the system further measure at different temperatures should be taken.
    
    \item \textbf{Magnetic susceptibility per spin}: The same consideration discussed for the heat capacity applies also to this observable.


\end{itemize}


